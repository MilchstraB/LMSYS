{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":8854274,"sourceType":"datasetVersion","datasetId":5329736},{"sourceId":8897601,"sourceType":"datasetVersion","datasetId":5297895},{"sourceId":8991206,"sourceType":"datasetVersion","datasetId":5415500},{"sourceId":9034194,"sourceType":"datasetVersion","datasetId":5445585}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers peft accelerate bitsandbytes \\\n    -U --no-index --find-links /kaggle/input/lmsys-wheel-files","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\nfrom typing import Dict, List, Optional, Union\n\nimport numpy as np\nfrom transformers import AutoTokenizer\n\n\ndef parse_text(text: str) -> list:\n    return eval(text, {\"null\": \"\"})\n\n\ntemplates_dict = {\n    \"chat_template_with_token_num\": \"\"\"<bos><start_of_turn>user\n{prompt}\\n\n<response_a> ({a_word_num} words): {response_a}\\n\n<response_b> ({b_word_num} words): {response_b}\n<end_of_turn>\n<start_of_turn>model\n\"\"\",\n    \"chat_template\": \"\"\"<bos><start_of_turn>user\n{prompt}\\n\n<response_a>: {response_a}\\n\n<response_b>: {response_b}\n<end_of_turn>\n<start_of_turn>model\n\"\"\",\n    \"template\": \"\"\"{prompt}\\n\n<response_a>: {response_a}\\n\n<response_b>: {response_b}\n<eos>\n\"\"\",\n    \"template_with_token_num\": \"\"\"{prompt}\\n\n<response_a> ({a_word_num} words): {response_a}\\n\n<response_b> ({b_word_num} words): {response_b}\n\"\"\",\n    \"template_with_token_num_eos\": \"\"\"{prompt}\\n\n<response_a> ({a_word_num} words): {response_a}\\n\n<response_b> ({b_word_num} words): {response_b}\n<eos>\n\"\"\",\n    \"template_with_eos\": \"\"\"{prompt}\\n\n<response_a>: {response_a}\\n\n<response_b>: {response_b}\n<eos>\n\"\"\",\n}\n\n\nclass TextProcessorV2:\n    def __init__(\n        self,\n        truncation_method: str,\n        length_assign_method: str,\n        tokenizer: AutoTokenizer,\n        max_length: int,\n        chat_template: Optional[str] = None,\n        get_labels: Optional[bool] = True,\n    ):\n        \"\"\"\n        Initializes the TextProcessor object.\n\n        Args:\n            truncation_method (str): The method used for truncating text.\n            length_assign_method (str): The method used for assigning length to text.\n            tokenizer (AutoTokenizer): The tokenizer object used for tokenization.\n            max_length (int): The maximum length of the processed text.\n            chat_template (Optional[str], optional): The chat template to be used. Defaults to None.\n            get_labels (Optional[bool], optional): Whether to retrieve labels. Defaults to True. [For Inference, set to False.]\n        \"\"\"\n        self.chat_template = templates_dict[\"chat_template_with_token_num\"]\n        if chat_template is not None:\n            self.chat_template = templates_dict[chat_template]\n\n        self.truncation_method = truncation_method\n        self.length_assign_method = length_assign_method\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.get_labels = get_labels\n\n    def preprocess_batch(\n        self, batch_data: Dict[str, List[str]]\n    ) -> Dict[str, List[str]]:\n        batch_prompt = [\" \".join(parse_text(t)).strip() for t in batch_data[\"prompt\"]]\n        batch_response_a = [\n            \" \".join(parse_text(t)).strip() for t in batch_data[\"response_a\"]\n        ]\n        batch_response_b = [\n            \" \".join(parse_text(t)).strip() for t in batch_data[\"response_b\"]\n        ]\n        return batch_prompt, batch_response_a, batch_response_b\n\n    def compute_token_num(self, text: str) -> int:\n        return len(\n            self.tokenizer(text, add_special_tokens=False, truncation=False)[\n                \"input_ids\"\n            ]\n        )\n\n    def format_texts(\n        self,\n        batch_prompt: List[str],\n        batch_response_a: List[str],\n        batch_response_b: List[str],\n        response_a_token_num: List[int],\n        response_b_token_num: List[int],\n    ) -> List[str]:\n        texts = []\n        for prompt, response_a, response_b, a_num, b_num in zip(\n            batch_prompt,\n            batch_response_a,\n            batch_response_b,\n            response_a_token_num,\n            response_b_token_num,\n        ):\n            if \"a_word_num\" in self.chat_template:\n                text = self.chat_template.format(\n                    prompt=prompt,\n                    response_a=response_a,\n                    response_b=response_b,\n                    a_word_num=a_num,\n                    b_word_num=b_num,\n                )\n            else:\n                text = self.chat_template.format(\n                    prompt=prompt,\n                    response_a=response_a,\n                    response_b=response_b,\n                )\n            texts.append(text)\n        return texts\n\n    def get_part_capacity(\n        self,\n        prompt_token_num: int,\n        response_a_token_num: int,\n        response_b_token_num: int,\n        cur_max_token_capacity: int,\n    ) -> tuple:\n        if self.length_assign_method == \"method_1\":\n            response_token_capacity = max(cur_max_token_capacity - prompt_token_num, 0)\n            prompt_capacity = min(prompt_token_num, cur_max_token_capacity)\n            response_a_capacity = int(\n                response_token_capacity\n                * response_a_token_num\n                / (response_a_token_num + response_b_token_num)\n            )\n            response_b_capacity = response_token_capacity - response_a_capacity\n        elif self.length_assign_method == \"method_2\":\n            total_tokens = (\n                prompt_token_num + response_a_token_num + response_b_token_num\n            )\n            prompt_capacity = int(\n                cur_max_token_capacity * prompt_token_num / total_tokens\n            )\n            response_a_capacity = int(\n                cur_max_token_capacity * response_a_token_num / total_tokens\n            )\n            response_b_capacity = (\n                cur_max_token_capacity - prompt_capacity - response_a_capacity\n            )\n        elif self.length_assign_method == \"method_3\":\n            response_token_capacity = max(cur_max_token_capacity - prompt_token_num, 0)\n            prompt_capacity = min(prompt_token_num, cur_max_token_capacity)\n            response_a_capacity = response_token_capacity // 2\n            response_b_capacity = response_token_capacity - response_a_capacity\n        else:\n            raise ValueError(\"Method not supported\")\n        return prompt_capacity, response_a_capacity, response_b_capacity\n\n    def __call__(self, batch_data):\n        \"\"\"\n        Preprocesses the text data in the batch and computes the token numbers for each part of the text.\n        Then, it calculates the maximum token capacity for the data and assigns token capacities to different parts of the text based on the specified length assignment method.\n        Finally, it truncates the text if necessary, encodes it into input_ids and attention_mask, and returns the final input.\n\n        Args:\n            batch_data (dict): A dictionary containing the batch data with keys \"prompt\", \"response_a\", and \"response_b\".\n\n        Returns:\n            dict: A dictionary containing the final input with keys \"input_ids\" and \"attention_mask\".\n\n        self.truncation_method 可以为 [left, right]，表示prompt从哪部分截断\n        self.length_assign_method 可以为 [method_1, method_2, method_3]，表示分配长度的方法\n            - 方法一：prompt全部保留，response_a和response_b按长度分配\n            - 方法二：prompt，response_a, response_b都按长度分配\n            - 方法三：prompt全部保留，response_a和response_b平分\n            - 方法四：原先的方法，直接截断response_b\n            ...\n        \"\"\"\n        batch_prompt, batch_response_a, batch_response_b = self.preprocess_batch(\n            batch_data\n        )\n        final_input = defaultdict(list)\n        if self.get_labels:\n            final_input[\"labels\"] = self.extract_labels(batch_data)\n        prompt_token_num = np.array([self.compute_token_num(p) for p in batch_prompt])\n        response_a_token_num = np.array(\n            [self.compute_token_num(r) for r in batch_response_a]\n        )\n        response_b_token_num = np.array(\n            [self.compute_token_num(r) for r in batch_response_b]\n        )\n        p_len, a_len, b_len = [], [], []\n        for i in range(len(batch_prompt)):\n            p_len.append(prompt_token_num[i])\n            a_len.append(response_a_token_num[i])\n            b_len.append(response_b_token_num[i])\n\n        final_input[\"original_prompt_length\"] = p_len\n        final_input[\"original_response_a_length\"] = a_len\n        final_input[\"original_response_b_length\"] = b_len\n        if self.length_assign_method == \"method_4\":\n            texts = self.format_texts(\n                batch_prompt,\n                batch_response_a,\n                batch_response_b,\n                response_a_token_num,\n                response_b_token_num,\n            )\n            tokenized = self.tokenizer(\n                texts,\n                max_length=self.max_length,\n                truncation=False,\n                add_special_tokens=False,\n            )\n            token_length = [len(t) for t in tokenized[\"input_ids\"]]\n\n            tokenized_truncation = self.tokenizer(\n                texts,\n                max_length=self.max_length,\n                truncation=True,\n                add_special_tokens=False,\n            )\n            for key in tokenized_truncation:\n                final_input[key] = tokenized_truncation[key]\n            final_input[\"token_length\"] = token_length\n            return final_input\n\n        concat_batch_text = self.format_texts(\n            batch_prompt,\n            batch_response_a,\n            batch_response_b,\n            response_a_token_num,\n            response_b_token_num,\n        )\n        concat_batch_text_token_num = np.array(\n            [self.compute_token_num(text) for text in concat_batch_text]\n        )\n\n        other_part_token_num = (\n            concat_batch_text_token_num\n            - prompt_token_num\n            - response_a_token_num\n            - response_b_token_num\n        )\n        max_token_capacity = self.max_length - other_part_token_num - 10\n        token_length = []\n        for i, token_num in enumerate(concat_batch_text_token_num):\n\n            if token_num > self.max_length:\n                prompt_capacity, response_a_capacity, response_b_capacity = (\n                    self.get_part_capacity(\n                        prompt_token_num[i],\n                        response_a_token_num[i],\n                        response_b_token_num[i],\n                        max_token_capacity[i],\n                    )\n                )\n                if self.truncation_method in [\"left\", \"right\"]:\n                    self.tokenizer.truncation_side = self.truncation_method\n                    prompt = self.tokenizer(\n                        batch_prompt[i],\n                        max_length=max(prompt_capacity, 0),\n                        truncation=True,\n                        add_special_tokens=False,\n                    )\n                    response_a = self.tokenizer(\n                        batch_response_a[i],\n                        max_length=max(response_a_capacity, 0),\n                        truncation=True,\n                        add_special_tokens=False,\n                    )\n                    response_b = self.tokenizer(\n                        batch_response_b[i],\n                        max_length=max(response_b_capacity, 0),\n                        truncation=True,\n                        add_special_tokens=False,\n                    )\n\n                    prompt_text = self.tokenizer.decode(prompt[\"input_ids\"]).strip()\n                    response_a_text = self.tokenizer.decode(\n                        response_a[\"input_ids\"]\n                    ).strip()\n                    response_b_text = self.tokenizer.decode(\n                        response_b[\"input_ids\"]\n                    ).strip()\n\n                    text = self.chat_template.format(\n                        prompt=prompt_text,\n                        response_a=response_a_text,\n                        response_b=response_b_text,\n                        a_word_num=response_a_token_num[i],\n                        b_word_num=response_b_token_num[i],\n                    )\n                else:\n                    raise ValueError(\"Truncation method not supported\")\n                inputs = self.tokenizer(\n                    text,\n                    max_length=self.max_length,\n                    truncation=False,\n                    add_special_tokens=False,\n                )\n                assert len(inputs[\"input_ids\"]) <= self.max_length\n                token_length.append(len(inputs[\"input_ids\"]))\n            else:\n                inputs = self.tokenizer(\n                    concat_batch_text[i],\n                    max_length=self.max_length,\n                    truncation=False,\n                    add_special_tokens=False,\n                )\n                assert len(inputs[\"input_ids\"]) <= self.max_length\n                token_length.append(len(inputs[\"input_ids\"]))\n            for key in inputs:\n                final_input[key].append(inputs[key])\n        final_input[\"token_length\"] = token_length\n        self.tokenizer.truncation_side = \"right\"\n\n        return final_input\n\n    def extract_labels(self, batch_data: Dict[str, List[str]]) -> List[int]:\n        labels = [\n            0 if a_win else 1 if b_win else 2\n            for a_win, b_win in zip(\n                batch_data[\"winner_model_a\"], batch_data[\"winner_model_b\"]\n            )\n        ]\n        return labels\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\nfrom concurrent.futures import ThreadPoolExecutor\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom peft import PeftModel\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom tqdm import trange\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n)\nfrom transformers.data.data_collator import pad_without_fast_tokenizer_warning\n\nfrom text_process import TextProcessorV2\n\ntorch.enable_grad(False)\n\n\n@dataclass\nclass Config:\n    model_name_or_path = \"/kaggle/input/gemma-2-9b-it/gemma2\"\n    lora_dir = \"/kaggle/input/gemma2it-loraforlmsys/gemma_lora_result/gemma_template_2e-4lr_right_truncation_method_2/checkpoint-1435\"\n    model_max_length = 2048\n    device_1 = torch.device(\"cuda:0\")\n    device_2 = torch.device(\"cuda:1\")\n    test_data_path = \"data/split/test.csv\"\n    default_chat_template = None\n    tta = True\n    test_local = False\n\n\ndef calculate_metrics(predictions_df, true_labels_df):\n    \"\"\"\n    Calculate log loss and accuracy between predictions and true labels.\n\n    Parameters:\n    predictions_df (pd.DataFrame): DataFrame containing predicted probabilities.\n    true_labels_df (pd.DataFrame): DataFrame containing true labels.\n\n    Returns:\n    tuple: (average log loss, accuracy)\n    \"\"\"\n    # Ensure the DataFrames are aligned on the index\n    predictions_df = predictions_df.set_index(\"id\").sort_index()\n    true_labels_df = true_labels_df.set_index(\"id\").sort_index()\n\n    # Extract true labels as one-hot encoded vectors\n    true_labels = true_labels_df[\n        [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n    ].values\n\n    # Extract predicted probabilities\n    predicted_probabilities = predictions_df[\n        [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n    ].values\n\n    # Calculate log loss\n    avg_log_loss = log_loss(true_labels, predicted_probabilities)\n\n    # Extract true labels as class indices for accuracy calculation\n    true_label_indices = np.argmax(true_labels, axis=1)\n    predicted_label_indices = np.argmax(predicted_probabilities, axis=1)\n\n    # Calculate accuracy\n    accuracy = accuracy_score(true_label_indices, predicted_label_indices)\n\n    return avg_log_loss, accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncfg = Config()\ntokenizer = AutoTokenizer.from_pretrained(\n    cfg.model_name_or_path, padding_side=\"right\", use_fast=True,\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npreprocess = TextProcessorV2(\n    tokenizer=tokenizer,\n    max_length=cfg.model_max_length,\n    chat_template=cfg.default_chat_template,\n    truncation_method=\"right\",\n    length_assign_method=\"method_2\"\n    get_labels=test_local\n)\n\nraw_dataset = Dataset.from_csv(cfg.test_data_path)\ntest_dataset = raw_dataset.map(preprocess, batched=True)\ntokenized_data = pd.DataFrame(test_dataset.to_dict())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess.chat_template","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntokenized_data[\"length\"] = tokenized_data[\"input_ids\"].apply(len)\nbnb_config =  BitsAndBytesConfig(load_in_8bit=True)\nmodel_1 = AutoModelForSequenceClassification.from_pretrained(\n    cfg.model_name_or_path,\n    num_labels=3,\n    device_map=cfg.device_1,\n    torch_dtype=torch.bfloat16,\n    bnb_config=bnb_config,\n)\nprint(model_1.score.weight)\nmodel_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir).eval()\nprint(model_1.score.weight)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2 = AutoModelForSequenceClassification.from_pretrained(\n    cfg.model_name_or_path,\n    num_labels=3,\n    device_map=cfg.device_2,\n    torch_dtype=torch.bfloat16,\n    bnb_config=bnb_config,\n)\nprint(model_2.score.weight)\nmodel_2 = PeftModel.from_pretrained(model_2, cfg.lora_dir).eval()\nprint(model_2.score.weight)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef predict(data, model, tokenizer, batch_size=1):\n    a_win, b_win, tie = [], [], []\n    predict_df = copy.deepcopy(data)\n\n    for i in trange(0, len(predict_df), batch_size):\n        end_idx = min(i + batch_size, len(predict_df))\n        tmp = predict_df.iloc[i:end_idx]\n        input_ids = tmp[\"input_ids\"].to_list()\n        attention_mask = tmp[\"attention_mask\"].to_list()\n        inputs = pad_without_fast_tokenizer_warning(\n                tokenizer,\n                {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n                padding=\"longest\",\n                pad_to_multiple_of=None,\n                return_tensors=\"pt\",\n            ).to(model.device)\n        outputs = model(**inputs)\n        proba = outputs.logits.softmax(-1).cpu()\n        a_win.extend(proba[:, 0].tolist())\n        b_win.extend(proba[:, 1].tolist())\n        tie.extend(proba[:, 2].tolist())\n    predict_df.loc[:, \"winner_model_a\"] = a_win\n    predict_df.loc[:, \"winner_model_b\"] = b_win\n    predict_df.loc[:, \"winner_tie\"] = tie\n    return predict_df[[\"id\", \"winner_model_a\", \"winner_model_b\",\"winner_tie\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sort_data = tokenized_data.sort_values(\"length\", ascending=False)\nsub_1 = sort_data.iloc[0::2].copy()\nsub_2 = sort_data.iloc[1::2].copy()\n\nwith ThreadPoolExecutor(max_workers=2) as executor:\n    results = executor.map(predict, (sub_1, sub_2), (model_1, model_2), (tokenizer, tokenizer), (4, 4))\n\nresult_df = pd.concat(list(results), axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if cfg.test_local:\n    print(calculate_metrics(result_df, tokenized_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TTA Test\n","metadata":{}},{"cell_type":"code","source":"aug_test_dataset = raw_dataset.rename_columns(\n    {\n        \"response_a\": \"response_b\",\n        \"response_b\": \"response_a\",\n        \"winner_model_a\": \"winner_model_b\",\n        \"winner_model_b\": \"winner_model_a\"\n    }\n)\naug_test_dataset = aug_test_dataset.map(preprocess, batched=True)\n\naug_tokenized_data = pd.DataFrame(aug_test_dataset.to_dict())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aug_tokenized_data.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_result = copy.deepcopy(result_df)\n\nif cfg.tta:\n    sort_aug_data = aug_tokenized_data.sort_values(\"token_length\", ascending=False)\n    sub_aug_1 = sort_aug_data.iloc[0::2].copy()\n    sub_aug_2 = sort_aug_data.iloc[1::2].copy()\n\n    with ThreadPoolExecutor(max_workers=2) as executor:\n        results = executor.map(predict, (sub_aug_1, sub_aug_2), (model_1, model_2), (tokenizer, tokenizer), (4, 4))\n\n    aug_result_df = pd.concat(list(results), axis=0)\n    if cfg.test_local:\n        print(calculate_metrics(aug_result_df, aug_tokenized_data))\n    aug_flip_result_df = copy.deepcopy(result_df)\n    aug_flip_result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]] = aug_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]] \n\n    final_result[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]] = (final_result[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]] + aug_flip_result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]]) / 2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_result.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if cfg.test_local:\n    print(calculate_metrics(final_result, tokenized_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}