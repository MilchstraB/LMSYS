{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import PeftModel\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from tqdm import trange\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "\n",
    "from text_process import TextProcessorV2\n",
    "\n",
    "torch.enable_grad(False)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name_or_path = \"/data/share/pyz/model_weight/gemma-2-9b-it\"\n",
    "    lora_dir = \"output/gemma_template_2e-4lr_right_truncation_method_2/checkpoint-1435\"\n",
    "    model_max_length = 2048\n",
    "    device_1 = torch.device(\"cuda:0\")\n",
    "    device_2 = torch.device(\"cuda:1\")\n",
    "    test_data_path = \"data/split/test.csv\"\n",
    "    default_chat_template = None\n",
    "    tta = True\n",
    "    test_local = False\n",
    "\n",
    "\n",
    "def calculate_metrics(predictions_df, true_labels_df):\n",
    "    \"\"\"\n",
    "    Calculate log loss and accuracy between predictions and true labels.\n",
    "\n",
    "    Parameters:\n",
    "    predictions_df (pd.DataFrame): DataFrame containing predicted probabilities.\n",
    "    true_labels_df (pd.DataFrame): DataFrame containing true labels.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (average log loss, accuracy)\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrames are aligned on the index\n",
    "    predictions_df = predictions_df.set_index(\"id\").sort_index()\n",
    "    true_labels_df = true_labels_df.set_index(\"id\").sort_index()\n",
    "\n",
    "    # Extract true labels as one-hot encoded vectors\n",
    "    true_labels = true_labels_df[\n",
    "        [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "    ].values\n",
    "\n",
    "    # Extract predicted probabilities\n",
    "    predicted_probabilities = predictions_df[\n",
    "        [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "    ].values\n",
    "\n",
    "    # Calculate log loss\n",
    "    avg_log_loss = log_loss(true_labels, predicted_probabilities)\n",
    "\n",
    "    # Extract true labels as class indices for accuracy calculation\n",
    "    true_label_indices = np.argmax(true_labels, axis=1)\n",
    "    predicted_label_indices = np.argmax(predicted_probabilities, axis=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_label_indices, predicted_label_indices)\n",
    "\n",
    "    return avg_log_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg = Config()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cfg.model_name_or_path, padding_side=\"right\", use_fast=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocess = TextProcessorV2(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=cfg.model_max_length,\n",
    "    chat_template=cfg.default_chat_template,\n",
    "    truncation_method=\"right\",\n",
    "    length_assign_method=\"method_2\"\n",
    ")\n",
    "\n",
    "raw_dataset = Dataset.from_csv(cfg.test_data_path)\n",
    "test_dataset = raw_dataset.map(preprocess, batched=True)\n",
    "tokenized_data = pd.DataFrame(test_dataset.to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_data[\"length\"] = tokenized_data[\"input_ids\"].apply(len)\n",
    "bnb_config =  BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "model_1 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    num_labels=3,\n",
    "    device_map=cfg.device_1,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "print(model_1.score.weight)\n",
    "model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir).eval()\n",
    "print(model_1.score.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    num_labels=3,\n",
    "    device_map=cfg.device_2,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "print(model_2.score.weight)\n",
    "model_2 = PeftModel.from_pretrained(model_2, cfg.lora_dir).eval()\n",
    "print(model_2.score.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict(data, model, tokenizer, batch_size=1):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "    predict_df = copy.deepcopy(data)\n",
    "\n",
    "    for i in trange(0, len(predict_df), batch_size):\n",
    "        end_idx = min(i + batch_size, len(predict_df))\n",
    "        tmp = predict_df.iloc[i:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "                tokenizer,\n",
    "                {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "                padding=\"longest\",\n",
    "                pad_to_multiple_of=None,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(model.device)\n",
    "        outputs = model(**inputs)\n",
    "        proba = outputs.logits.softmax(-1).cpu()\n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    predict_df.loc[:, \"winner_model_a\"] = a_win\n",
    "    predict_df.loc[:, \"winner_model_b\"] = b_win\n",
    "    predict_df.loc[:, \"winner_tie\"] = tie\n",
    "    return predict_df[[\"id\", \"winner_model_a\", \"winner_model_b\",\"winner_tie\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_data = tokenized_data.sort_values(\"length\", ascending=False)\n",
    "sub_1 = sort_data.iloc[0::2].copy()\n",
    "sub_2 = sort_data.iloc[1::2].copy()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(predict, (sub_1, sub_2), (model_1, model_2), (tokenizer, tokenizer), (4, 4))\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.test_local:\n",
    "    print(calculate_metrics(result_df, tokenized_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTA Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_test_dataset = raw_dataset.rename_columns(\n",
    "    {\n",
    "        \"response_a\": \"response_b\",\n",
    "        \"response_b\": \"response_a\",\n",
    "        \"winner_model_a\": \"winner_model_b\",\n",
    "        \"winner_model_b\": \"winner_model_a\"\n",
    "    }\n",
    ")\n",
    "aug_test_dataset = aug_test_dataset.map(preprocess, batched=True)\n",
    "\n",
    "aug_tokenized_data = pd.DataFrame(aug_test_dataset.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_tokenized_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = copy.deepcopy(result_df)\n",
    "\n",
    "if cfg.tta:\n",
    "    sort_aug_data = aug_tokenized_data.sort_values(\"token_length\", ascending=False)\n",
    "    sub_aug_1 = sort_aug_data.iloc[0::2].copy()\n",
    "    sub_aug_2 = sort_aug_data.iloc[1::2].copy()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        results = executor.map(predict, (sub_aug_1, sub_aug_2), (model_1, model_2), (tokenizer, tokenizer), (4, 4))\n",
    "\n",
    "    aug_result_df = pd.concat(list(results), axis=0)\n",
    "    if cfg.test_local:\n",
    "        print(calculate_metrics(aug_result_df, aug_tokenized_data))\n",
    "    aug_flip_result_df = copy.deepcopy(result_df)\n",
    "    aug_flip_result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]] = aug_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]] \n",
    "\n",
    "    final_result[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]] = (final_result[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]] + aug_flip_result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.test_local:\n",
    "    print(calculate_metrics(final_result, tokenized_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl_compression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
