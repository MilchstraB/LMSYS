{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["!pip install transformers peft accelerate bitsandbytes torch \\\n","    -U --no-index --find-links /kaggle/input/lmsys-wheel-files"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip show torch"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","print(torch.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import json\n","from collections import defaultdict\n","from typing import Dict, List, Optional, Union\n","\n","import numpy as np\n","from transformers import AutoTokenizer\n","\n","\n","def parse_text(text: str) -> list:\n","    return eval(text, {\"null\": \"\"})\n","\n","\n","class TextProcessorV2:\n","    def __init__(\n","        self,\n","        truncation_method: str,\n","        length_assign_method: str,\n","        tokenizer: AutoTokenizer,\n","        max_length: int,\n","        chat_template: Optional[str] = None,\n","        get_labels: Optional[bool] = True,\n","    ):\n","        \"\"\"\n","        Initializes the TextProcessor object.\n","\n","        Args:\n","            truncation_method (str): The method used for truncating text.\n","            length_assign_method (str): The method used for assigning length to text.\n","            tokenizer (AutoTokenizer): The tokenizer object used for tokenization.\n","            max_length (int): The maximum length of the processed text.\n","            chat_template (Optional[str], optional): The chat template to be used. Defaults to None.\n","            get_labels (Optional[bool], optional): Whether to retrieve labels. Defaults to True. [For Inference, set to False.]\n","        \"\"\"\n","        self.chat_template = templates_dict[\"chat_template_with_token_num\"]\n","        if chat_template is not None and chat_template in templates_dict:\n","            self.chat_template = templates_dict[chat_template]\n","        elif isinstance(chat_template, str):\n","            self.chat_template = chat_template\n","            print(f\"[WEARING]: The chat_template set as: {self.chat_template}\")\n","        else:\n","            raise ValueError(\"Chat template not supported\")\n","        self.truncation_method = truncation_method\n","        self.length_assign_method = length_assign_method\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.get_labels = get_labels\n","\n","    def preprocess_batch(\n","        self, batch_data: Dict[str, List[str]]\n","    ) -> Dict[str, List[str]]:\n","        batch_prompt = [\" \".join(parse_text(t)).strip() for t in batch_data[\"prompt\"]]\n","        batch_response_a = [\n","            \" \".join(parse_text(t)).strip() for t in batch_data[\"response_a\"]\n","        ]\n","        batch_response_b = [\n","            \" \".join(parse_text(t)).strip() for t in batch_data[\"response_b\"]\n","        ]\n","        return batch_prompt, batch_response_a, batch_response_b\n","\n","    def compute_token_num(self, text: str) -> int:\n","        return len(\n","            self.tokenizer(text, add_special_tokens=False, truncation=False)[\n","                \"input_ids\"\n","            ]\n","        )\n","\n","    def format_texts(\n","        self,\n","        batch_prompt: List[str],\n","        batch_response_a: List[str],\n","        batch_response_b: List[str],\n","        response_a_token_num: List[int],\n","        response_b_token_num: List[int],\n","    ) -> List[str]:\n","        texts = []\n","        for prompt, response_a, response_b, a_num, b_num in zip(\n","            batch_prompt,\n","            batch_response_a,\n","            batch_response_b,\n","            response_a_token_num,\n","            response_b_token_num,\n","        ):\n","            if \"a_word_num\" in self.chat_template:\n","                text = self.chat_template.format(\n","                    prompt=prompt,\n","                    response_a=response_a,\n","                    response_b=response_b,\n","                    a_word_num=a_num,\n","                    b_word_num=b_num,\n","                )\n","            else:\n","                text = self.chat_template.format(\n","                    prompt=prompt,\n","                    response_a=response_a,\n","                    response_b=response_b,\n","                )\n","            texts.append(text)\n","        return texts\n","\n","    def get_part_capacity(\n","        self,\n","        prompt_token_num: int,\n","        response_a_token_num: int,\n","        response_b_token_num: int,\n","        cur_max_token_capacity: int,\n","    ) -> tuple:\n","        if self.length_assign_method == \"method_1\":\n","            response_token_capacity = max(cur_max_token_capacity - prompt_token_num, 0)\n","            prompt_capacity = min(prompt_token_num, cur_max_token_capacity)\n","            response_a_capacity = int(\n","                response_token_capacity\n","                * response_a_token_num\n","                / (response_a_token_num + response_b_token_num)\n","            )\n","            response_b_capacity = response_token_capacity - response_a_capacity\n","        elif self.length_assign_method == \"method_2\":\n","            total_tokens = (\n","                prompt_token_num + response_a_token_num + response_b_token_num\n","            )\n","            prompt_capacity = int(\n","                cur_max_token_capacity * prompt_token_num / total_tokens\n","            )\n","            response_a_capacity = int(\n","                cur_max_token_capacity * response_a_token_num / total_tokens\n","            )\n","            response_b_capacity = (\n","                cur_max_token_capacity - prompt_capacity - response_a_capacity\n","            )\n","        elif self.length_assign_method == \"method_3\":\n","            response_token_capacity = max(cur_max_token_capacity - prompt_token_num, 0)\n","            prompt_capacity = min(prompt_token_num, cur_max_token_capacity)\n","            response_a_capacity = response_token_capacity // 2\n","            response_b_capacity = response_token_capacity - response_a_capacity\n","        else:\n","            raise ValueError(\"Method not supported\")\n","        return prompt_capacity, response_a_capacity, response_b_capacity\n","\n","    def __call__(self, batch_data):\n","        \"\"\"\n","        Preprocesses the text data in the batch and computes the token numbers for each part of the text.\n","        Then, it calculates the maximum token capacity for the data and assigns token capacities to different parts of the text based on the specified length assignment method.\n","        Finally, it truncates the text if necessary, encodes it into input_ids and attention_mask, and returns the final input.\n","\n","        Args:\n","            batch_data (dict): A dictionary containing the batch data with keys \"prompt\", \"response_a\", and \"response_b\".\n","\n","        Returns:\n","            dict: A dictionary containing the final input with keys \"input_ids\" and \"attention_mask\".\n","\n","        self.truncation_method 可以为 [left, right]，表示prompt从哪部分截断\n","        self.length_assign_method 可以为 [method_1, method_2, method_3]，表示分配长度的方法\n","            - 方法一：prompt全部保留，response_a和response_b按长度分配\n","            - 方法二：prompt，response_a, response_b都按长度分配\n","            - 方法三：prompt全部保留，response_a和response_b平分\n","            - 方法四：原先的方法，直接截断response_b\n","            ...\n","        \"\"\"\n","        batch_prompt, batch_response_a, batch_response_b = self.preprocess_batch(\n","            batch_data\n","        )\n","        final_input = defaultdict(list)\n","        if self.get_labels:\n","            final_input[\"labels\"] = self.extract_labels(batch_data)\n","        prompt_token_num = np.array([self.compute_token_num(p) for p in batch_prompt])\n","        response_a_token_num = np.array(\n","            [self.compute_token_num(r) for r in batch_response_a]\n","        )\n","        response_b_token_num = np.array(\n","            [self.compute_token_num(r) for r in batch_response_b]\n","        )\n","        p_len, a_len, b_len = [], [], []\n","        for i in range(len(batch_prompt)):\n","            p_len.append(prompt_token_num[i])\n","            a_len.append(response_a_token_num[i])\n","            b_len.append(response_b_token_num[i])\n","\n","        final_input[\"original_prompt_length\"] = p_len\n","        final_input[\"original_response_a_length\"] = a_len\n","        final_input[\"original_response_b_length\"] = b_len\n","        if self.length_assign_method == \"method_4\":\n","            texts = self.format_texts(\n","                batch_prompt,\n","                batch_response_a,\n","                batch_response_b,\n","                response_a_token_num,\n","                response_b_token_num,\n","            )\n","            tokenized = self.tokenizer(\n","                texts,\n","                max_length=self.max_length,\n","                truncation=False,\n","                add_special_tokens=False,\n","            )\n","            token_length = [len(t) for t in tokenized[\"input_ids\"]]\n","\n","            tokenized_truncation = self.tokenizer(\n","                texts,\n","                max_length=self.max_length,\n","                truncation=True,\n","                add_special_tokens=False,\n","            )\n","            for key in tokenized_truncation:\n","                final_input[key] = tokenized_truncation[key]\n","            final_input[\"token_length\"] = token_length\n","            return final_input\n","\n","        concat_batch_text = self.format_texts(\n","            batch_prompt,\n","            batch_response_a,\n","            batch_response_b,\n","            response_a_token_num,\n","            response_b_token_num,\n","        )\n","        concat_batch_text_token_num = np.array(\n","            [self.compute_token_num(text) for text in concat_batch_text]\n","        )\n","\n","        other_part_token_num = (\n","            concat_batch_text_token_num\n","            - prompt_token_num\n","            - response_a_token_num\n","            - response_b_token_num\n","        )\n","        max_token_capacity = self.max_length - other_part_token_num - 10\n","        token_length = []\n","        for i, token_num in enumerate(concat_batch_text_token_num):\n","\n","            if token_num > self.max_length:\n","                prompt_capacity, response_a_capacity, response_b_capacity = (\n","                    self.get_part_capacity(\n","                        prompt_token_num[i],\n","                        response_a_token_num[i],\n","                        response_b_token_num[i],\n","                        max_token_capacity[i],\n","                    )\n","                )\n","                if self.truncation_method in [\"left\", \"right\"]:\n","                    self.tokenizer.truncation_side = self.truncation_method\n","                    prompt = self.tokenizer(\n","                        batch_prompt[i],\n","                        max_length=max(prompt_capacity, 0),\n","                        truncation=True,\n","                        add_special_tokens=False,\n","                    )\n","                    response_a = self.tokenizer(\n","                        batch_response_a[i],\n","                        max_length=max(response_a_capacity, 0),\n","                        truncation=True,\n","                        add_special_tokens=False,\n","                    )\n","                    response_b = self.tokenizer(\n","                        batch_response_b[i],\n","                        max_length=max(response_b_capacity, 0),\n","                        truncation=True,\n","                        add_special_tokens=False,\n","                    )\n","\n","                    prompt_text = self.tokenizer.decode(prompt[\"input_ids\"]).strip()\n","                    response_a_text = self.tokenizer.decode(\n","                        response_a[\"input_ids\"]\n","                    ).strip()\n","                    response_b_text = self.tokenizer.decode(\n","                        response_b[\"input_ids\"]\n","                    ).strip()\n","\n","                    text = self.chat_template.format(\n","                        prompt=prompt_text,\n","                        response_a=response_a_text,\n","                        response_b=response_b_text,\n","                        a_word_num=response_a_token_num[i],\n","                        b_word_num=response_b_token_num[i],\n","                    )\n","                else:\n","                    raise ValueError(\"Truncation method not supported\")\n","                inputs = self.tokenizer(\n","                    text,\n","                    max_length=self.max_length,\n","                    truncation=False,\n","                    add_special_tokens=False,\n","                )\n","                assert len(inputs[\"input_ids\"]) <= self.max_length\n","                token_length.append(len(inputs[\"input_ids\"]))\n","            else:\n","                inputs = self.tokenizer(\n","                    concat_batch_text[i],\n","                    max_length=self.max_length,\n","                    truncation=False,\n","                    add_special_tokens=False,\n","                )\n","                assert len(inputs[\"input_ids\"]) <= self.max_length\n","                token_length.append(len(inputs[\"input_ids\"]))\n","            for key in inputs:\n","                final_input[key].append(inputs[key])\n","        final_input[\"token_length\"] = token_length\n","        self.tokenizer.truncation_side = \"right\"\n","\n","        return final_input\n","\n","    def extract_labels(self, batch_data: Dict[str, List[str]]) -> List[int]:\n","        labels = [\n","            0 if a_win else 1 if b_win else 2\n","            for a_win, b_win in zip(\n","                batch_data[\"winner_model_a\"], batch_data[\"winner_model_b\"]\n","            )\n","        ]\n","        return labels"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import copy\n","import os\n","from concurrent.futures import ThreadPoolExecutor\n","from dataclasses import dataclass\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","from datasets import Dataset\n","from peft import PeftModel\n","from sklearn.metrics import accuracy_score, log_loss\n","from tqdm import trange\n","from transformers import (\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n",")\n","from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n","\n","torch.enable_grad(False)\n","\n","\n","@dataclass\n","class Config:\n","    # Fix\n","    model_name_or_path = \"/kaggle/input/gemma-2-9b-it/gemma2\"\n","    device_1 = torch.device(\"cuda:0\")\n","    device_2 = torch.device(\"cuda:1\")\n","    # Maybe change:\n","    test_data_path = \"/kaggle/input/lmsys-chatbot-arena/test.csv\"\n","    test_local = False\n","    # Need Change:\n","    lora_dir = \"/kaggle/input/gemma2it-loraforlmsys/gemma_lora_result/gemma_template_2e-4lr_right_truncation_method_2/checkpoint-1435\"\n","    model_max_length = 2048\n","    default_chat_template = None\n","    tta = True\n","    truncation_method = \"right\"\n","    length_assign_method = \"method_2\"\n","    hyper_parameter = None\n","\n","\n","def calculate_metrics(predictions_df, true_labels_df):\n","    \"\"\"\n","    Calculate log loss and accuracy between predictions and true labels.\n","\n","    Parameters:\n","    predictions_df (pd.DataFrame): DataFrame containing predicted probabilities.\n","    true_labels_df (pd.DataFrame): DataFrame containing true labels.\n","\n","    Returns:\n","    tuple: (average log loss, accuracy)\n","    \"\"\"\n","    # Ensure the DataFrames are aligned on the index\n","    predictions_df = predictions_df.set_index(\"id\").sort_index()\n","    true_labels_df = true_labels_df.set_index(\"id\").sort_index()\n","\n","    # Extract true labels as one-hot encoded vectors\n","    true_labels = true_labels_df[\n","        [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n","    ].values\n","\n","    # Extract predicted probabilities\n","    predicted_probabilities = predictions_df[\n","        [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n","    ].values\n","\n","    # Calculate log loss\n","    avg_log_loss = log_loss(true_labels, predicted_probabilities)\n","\n","    # Extract true labels as class indices for accuracy calculation\n","    true_label_indices = np.argmax(true_labels, axis=1)\n","    predicted_label_indices = np.argmax(predicted_probabilities, axis=1)\n","\n","    # Calculate accuracy\n","    accuracy = accuracy_score(true_label_indices, predicted_label_indices)\n","\n","    return avg_log_loss, accuracy\n","\n","\n","cfg = Config()\n","hyper_parameter_path = f\"{os.path.dirname(cfg.lora_dir)}/hyper_parameter.json\"\n","if os.path.exists(hyper_parameter_path):\n","    hyper_parameter = json.load(open())\n","    cfg.truncation_method = hyper_parameter['truncation_method']\n","    cfg.length_assign_method = hyper_parameter['length_assign_method']\n","    cfg.default_chat_template = hyper_parameter['chat_template']\n","    cfg.model_max_length = hyper_parameter['model_max_length']\n","    print(\"Hyper parameter loaded.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\n","    cfg.model_name_or_path, padding_side=\"right\", use_fast=True,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","preprocess = TextProcessorV2(\n","    tokenizer=tokenizer,\n","    max_length=cfg.model_max_length,\n","    chat_template=cfg.default_chat_template,\n","    truncation_method=cfg.truncation_method,\n","    length_assign_method=cfg.length_assign_method,\n","    get_labels=cfg.test_local\n",")\n","\n","raw_dataset = Dataset.from_csv(cfg.test_data_path)\n","test_dataset = raw_dataset.map(preprocess, batched=True)\n","tokenized_data = pd.DataFrame(test_dataset.to_dict())\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["preprocess.chat_template"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","tokenized_data[\"length\"] = tokenized_data[\"input_ids\"].apply(len)\n","bnb_config =  BitsAndBytesConfig(load_in_8bit=True)\n","model_1 = AutoModelForSequenceClassification.from_pretrained(\n","    cfg.model_name_or_path,\n","    num_labels=3,\n","    device_map=cfg.device_1,\n","    low_cpu_mem_usage=True,\n","#     torch_dtype=torch.bfloat16,\n","    quantization_config=bnb_config,\n",")\n","print(model_1.score.weight)\n","model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir).eval()\n","print(model_1.score.weight)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_2 = AutoModelForSequenceClassification.from_pretrained(\n","    cfg.model_name_or_path,\n","    num_labels=3,\n","    device_map=cfg.device_2,\n","    low_cpu_mem_usage=True,\n","#     torch_dtype=torch.bfloat16,\n","    quantization_config=bnb_config,\n",")\n","print(model_2.score.weight)\n","model_2 = PeftModel.from_pretrained(model_2, cfg.lora_dir).eval()\n","print(model_2.score.weight)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","\n","def predict(data, model, tokenizer, batch_size=1):\n","    a_win, b_win, tie = [], [], []\n","    predict_df = copy.deepcopy(data)\n","\n","    for i in trange(0, len(predict_df), batch_size):\n","        end_idx = min(i + batch_size, len(predict_df))\n","        tmp = predict_df.iloc[i:end_idx]\n","        input_ids = tmp[\"input_ids\"].to_list()\n","        attention_mask = tmp[\"attention_mask\"].to_list()\n","        inputs = pad_without_fast_tokenizer_warning(\n","                tokenizer,\n","                {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n","                padding=\"longest\",\n","                pad_to_multiple_of=None,\n","                return_tensors=\"pt\",\n","            ).to(model.device)\n","        outputs = model(**inputs)\n","        proba = outputs.logits.softmax(-1).cpu()\n","        a_win.extend(proba[:, 0].tolist())\n","        b_win.extend(proba[:, 1].tolist())\n","        tie.extend(proba[:, 2].tolist())\n","    predict_df.loc[:, \"winner_model_a\"] = a_win\n","    predict_df.loc[:, \"winner_model_b\"] = b_win\n","    predict_df.loc[:, \"winner_tie\"] = tie\n","    return predict_df[[\"id\", \"winner_model_a\", \"winner_model_b\",\"winner_tie\"]]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sort_data = tokenized_data.sort_values(\"length\", ascending=False)\n","sub_1 = sort_data.iloc[0::2].copy()\n","sub_2 = sort_data.iloc[1::2].copy()\n","\n","with ThreadPoolExecutor(max_workers=2) as executor:\n","    results = executor.map(predict, (sub_1, sub_2), (model_1, model_2), (tokenizer, tokenizer), (4, 4))\n","    \n","result_df = pd.concat(list(results), axis=0)\n","results = result_df.sort_values(\"id\")\n","results.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if cfg.test_local:\n","    print(calculate_metrics(result_df, tokenized_data))"]},{"cell_type":"markdown","metadata":{},"source":["# TTA Test\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["aug_test_dataset = raw_dataset.rename_columns(\n","    {\n","        \"response_a\": \"response_b\",\n","        \"response_b\": \"response_a\",\n","    }\n",")\n","if cfg.test_local:\n","    aug_test_dataset = aug_test_dataset.rename_columns(\n","        {\n","            \"winner_model_a\": \"winner_model_b\",\n","            \"winner_model_b\": \"winner_model_a\"\n","        }\n","    )\n","            \n","aug_test_dataset = aug_test_dataset.map(preprocess, batched=True)\n","\n","aug_tokenized_data = pd.DataFrame(aug_test_dataset.to_dict())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["aug_tokenized_data.head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["final_result = copy.deepcopy(result_df)\n","\n","if cfg.tta:\n","    sort_aug_data = aug_tokenized_data.sort_values(\"token_length\", ascending=False)\n","    sub_aug_1 = sort_aug_data.iloc[0::2].copy()\n","    sub_aug_2 = sort_aug_data.iloc[1::2].copy()\n","\n","    with ThreadPoolExecutor(max_workers=2) as executor:\n","        results = executor.map(predict, (sub_aug_1, sub_aug_2), (model_1, model_2), (tokenizer, tokenizer), (4, 4))\n","\n","    aug_result_df = pd.concat(list(results), axis=0)\n","    aug_result_df = aug_result_df.sort_values(\"id\")\n","    if cfg.test_local:\n","        print(calculate_metrics(aug_result_df, aug_tokenized_data))\n","    final_result[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]] = (final_result[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]] + aug_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]] ) / 2\n","    display(aug_result_df.head(5))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["result_df.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["final_result.to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if cfg.test_local:\n","    print(calculate_metrics(final_result, tokenized_data))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8346466,"sourceId":66631,"sourceType":"competition"},{"datasetId":5329736,"sourceId":8854274,"sourceType":"datasetVersion"},{"datasetId":5297895,"sourceId":8897601,"sourceType":"datasetVersion"},{"datasetId":5415500,"sourceId":8991206,"sourceType":"datasetVersion"},{"datasetId":5445585,"sourceId":9034194,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
